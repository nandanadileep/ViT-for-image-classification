{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":7615428,"datasetId":4434986,"databundleVersionId":7711037,"isSourceIdPinned":false}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade --quiet transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T18:25:50.046514Z","iopub.execute_input":"2025-07-28T18:25:50.047207Z","iopub.status.idle":"2025-07-28T18:26:05.745967Z","shell.execute_reply.started":"2025-07-28T18:25:50.047176Z","shell.execute_reply":"2025-07-28T18:26:05.745082Z"}},"outputs":[{"name":"stdout","text":"4.52.4\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport pandas as pd\nfrom tqdm import tqdm\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split\nfrom sklearn.metrics import accuracy_score, f1_score\nimport matplotlib.pyplot as plt\nimport kagglehub\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T18:28:02.429291Z","iopub.execute_input":"2025-07-28T18:28:02.430139Z","iopub.status.idle":"2025-07-28T18:28:14.331603Z","shell.execute_reply.started":"2025-07-28T18:28:02.430113Z","shell.execute_reply":"2025-07-28T18:28:14.331010Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import kagglehub\npath = kagglehub.dataset_download(\"hungle3401/faceforensics\")\nprint(\"Dataset path:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T18:49:23.428071Z","iopub.execute_input":"2025-07-28T18:49:23.429110Z","iopub.status.idle":"2025-07-28T18:49:23.574390Z","shell.execute_reply.started":"2025-07-28T18:49:23.429079Z","shell.execute_reply":"2025-07-28T18:49:23.573795Z"}},"outputs":[{"name":"stdout","text":"Dataset path: /kaggle/input/faceforensics\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"#extracting images from the video\n\nimport os\nimport cv2\nfrom tqdm import tqdm\n\nreal_video_dir = os.path.join(path, \"FF++\", \"real\")\nfake_video_dir = os.path.join(path, \"FF++\", \"fake\")\noutput_real = '/kaggle/working/frames/real'\noutput_fake = '/kaggle/working/frames/fake'\n\ndef extract_frames_from_videos(video_dir, output_dir, label, frames_per_video=5):\n    os.makedirs(output_dir, exist_ok=True)\n    video_files = [f for f in os.listdir(video_dir) if f.endswith('.mp4')]\n    for video_file in tqdm(video_files, desc=f\"Extracting {label}\"):\n        video_path = os.path.join(video_dir, video_file)\n        cap = cv2.VideoCapture(video_path)\n        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        step = max(1, frame_count // frames_per_video)\n        for i in range(frames_per_video):\n            cap.set(cv2.CAP_PROP_POS_FRAMES, i * step)\n            ret, frame = cap.read()\n            if ret:\n                img_name = f\"{label}_{os.path.splitext(video_file)[0]}_frame{i}.jpg\"\n                img_path = os.path.join(output_dir, img_name)\n                cv2.imwrite(img_path, frame)\n        cap.release()\n\nextract_frames_from_videos(real_video_dir, output_real, 'real')\nextract_frames_from_videos(fake_video_dir, output_fake, 'fake')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T18:51:39.046817Z","iopub.execute_input":"2025-07-28T18:51:39.047674Z","iopub.status.idle":"2025-07-28T19:03:50.350576Z","shell.execute_reply.started":"2025-07-28T18:51:39.047646Z","shell.execute_reply":"2025-07-28T19:03:50.349873Z"}},"outputs":[{"name":"stderr","text":"Extracting real: 100%|██████████| 200/200 [06:04<00:00,  1.82s/it]\nExtracting fake: 100%|██████████| 200/200 [06:06<00:00,  1.83s/it]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from transformers import ViTForImageClassification, ViTImageProcessor, TrainingArguments, Trainer\nfrom torch.utils.data import Dataset, random_split\nfrom PIL import Image\nimport torch\nimport numpy as np\n\nclass FaceForensicsDataset(Dataset):\n    def __init__(self, root_dir, processor):\n        self.samples = []\n        self.processor = processor\n        for label, subfolder in enumerate(['real', 'fake']):\n            folder = os.path.join(root_dir, subfolder)\n            for fname in os.listdir(folder):\n                if fname.endswith('.jpg'):\n                    self.samples.append((os.path.join(folder, fname), label))\n    def __len__(self):\n        return len(self.samples)\n    def __getitem__(self, idx):\n        img_path, label = self.samples[idx]\n        image = Image.open(img_path).convert('RGB')\n        processed = self.processor(images=image, return_tensors=\"pt\")\n        item = {k: v.squeeze(0) for k, v in processed.items()}\n        item['labels'] = torch.tensor(label)\n        return item\n\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\ndataset = FaceForensicsDataset('/kaggle/working/frames', processor)\n\n# Split into train/val\nval_pct = 0.2\nval_size = int(len(dataset) * val_pct)\ntrain_size = len(dataset) - val_size\ntrain_ds, val_ds = random_split(dataset, [train_size, val_size])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T19:15:35.328058Z","iopub.execute_input":"2025-07-28T19:15:35.328380Z","iopub.status.idle":"2025-07-28T19:15:35.512632Z","shell.execute_reply.started":"2025-07-28T19:15:35.328355Z","shell.execute_reply":"2025-07-28T19:15:35.512078Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"model = ViTForImageClassification.from_pretrained(\n    'google/vit-base-patch16-224-in21k',\n    num_labels=2,\n    id2label={0: 'real', 1: 'fake'},\n    label2id={'real': 0, 'fake': 1}\n)\n\nfrom sklearn.metrics import accuracy_score, f1_score\n\ndef collate_fn(batch):\n    return {\n        'pixel_values': torch.stack([item['pixel_values'] for item in batch]),\n        'labels': torch.tensor([item['labels'] for item in batch])\n    }\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=1)\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1\": f1_score(labels, preds)\n    }\n\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='./vit-ff',\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=5,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_steps=10,\n    learning_rate=2e-5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_accuracy\",\n    report_to=\"none\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=val_ds,\n    data_collator=collate_fn,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T19:19:13.689852Z","iopub.execute_input":"2025-07-28T19:19:13.690135Z","iopub.status.idle":"2025-07-28T19:27:15.575936Z","shell.execute_reply.started":"2025-07-28T19:19:13.690115Z","shell.execute_reply":"2025-07-28T19:27:15.575287Z"}},"outputs":[{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 07:55, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.617900</td>\n      <td>0.574290</td>\n      <td>0.735000</td>\n      <td>0.765487</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.475800</td>\n      <td>0.522705</td>\n      <td>0.757500</td>\n      <td>0.789588</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.479000</td>\n      <td>0.514581</td>\n      <td>0.742500</td>\n      <td>0.770601</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.374600</td>\n      <td>0.519964</td>\n      <td>0.725000</td>\n      <td>0.746544</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.417600</td>\n      <td>0.526852</td>\n      <td>0.725000</td>\n      <td>0.745370</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=500, training_loss=0.4831079092025757, metrics={'train_runtime': 480.7606, 'train_samples_per_second': 16.64, 'train_steps_per_second': 1.04, 'total_flos': 6.19935916916736e+17, 'train_loss': 0.4831079092025757, 'epoch': 5.0})"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"eval_results = trainer.evaluate()\nprint(\"Validation results:\", eval_results)\n\ndef predict_vit(image_path, model, processor, device='cuda'):\n    image = Image.open(image_path).convert('RGB')\n    inputs = processor(images=image, return_tensors=\"pt\")\n    model = model.to(device)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    model.eval()\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        pred = logits.argmax(dim=1).item()\n    return 'real' if pred == 0 else 'fake'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T19:40:30.424647Z","iopub.execute_input":"2025-07-28T19:40:30.425316Z","iopub.status.idle":"2025-07-28T19:40:44.602880Z","shell.execute_reply.started":"2025-07-28T19:40:30.425291Z","shell.execute_reply":"2025-07-28T19:40:44.602170Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [25/25 00:13]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Validation results: {'eval_loss': 0.5227053761482239, 'eval_accuracy': 0.7575, 'eval_f1': 0.7895878524945771, 'eval_runtime': 14.169, 'eval_samples_per_second': 28.231, 'eval_steps_per_second': 1.764, 'epoch': 5.0}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}